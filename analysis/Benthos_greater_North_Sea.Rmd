---
title: "Benthos_greater_North_Sea"
author: "Peter M.J. Herman, Willem Stolte, Luuk van der Heijden"
date: "26-1-2021"
knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file="../docs/Benthos_greater_North_Sea.pdf")}) 
output: pdf_document
---
## Introduction

The large databases of EMODNET Biology only store confirmed presences of species. However, when mapping species distribution, it is also important where the species did not occur: there is at least as much information in absences as in presences. Inferring absences from presence-only databases is difficult and always involves some guesswork. In this product we have used as much meta-information as possible to guide us in inferring absences. There is important meta-information at two different levels: the level of the data set, and the level of the species. Datasets can contain implicit information on absences when they have uniformly searched for the same species over a number of sample locations. Normally, if the species would have been present there, it would have been recorded. Other datasets, however, are not informative at all about absences. Typical examples are museum collections. The fact that a specimen is found at a particular place confirms that it lived there, but does not give information on any other species being present or absent in the same spot. A difficulty is that some datasets have searched for a restricted part of the total community, e.g. only sampled shellfish but no worms. In this case, absence of a shellfish species is relevant, but absence of a worm is not. The dataset can only be used to infer absence for the species it has targeted. Here we implicitly assume that a dataset inventoring the endomacrobenthos, is targeting all species belonging to this functional group. Usually, the distinction can be made on the basis of the metadata. It is also helpful to plot the total number of species versus the total number of samples. Incomplete datasets have far less species than expected for their size, compared to 'complete' datasets. At the species level, taxonomic databases such as WoRMS give information on the functional group the species belongs to. This information is present for many species, but it is incomplete. However, we can use it in a step to select the most useful datasets. We could use it to restrict the species to be used in mapping, but since the data are incomplete that would cause some loss of interesting information.

```{r setup, eval=FALSE}
options(tinytex.verbose = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
require(raster)
require(sp)
require(rgdal)
require(RColorBrewer)
require(svMisc)
require(tidyverse)
require(sf)
require(worrms)
library(ggplot2)
library('rnaturalearth')
library(magick)
library(rgeos)
library(EMODnetBiologyMaps)
require(rgdal)
require(imis)
require(progress)

#############
downloadDir <- "data/raw_data"
dataDir     <- "data/derived_data"
mapsDir     <- "product/maps"
rasterDir   <- "product/species_rasters"
plotsDir    <- "product/species_plots"
rareDir     <- "product/rarespecies"
scriptsDir  <- "scripts"
#############
proWG<-CRS("+proj=longlat +datum=WGS84")
##########################################################
# define a raster covering the grid. Set resolution of the raster here
##########################################################
r<-raster(ext=extent(-16,9,46,66),ncol=150,nrow=280,crs=proWG,vals=0)
```


## Data selection and retrieval

The retrieval of data goes in three steps. 

### Step 1. Use functional group information to harvest potentially interesting datasets. 

A query is performed for data on species known to be benthic (in WoRMS) and to occur in a number of different sea regions. This yields a large dataset with benthic data, but many of these data come from datasets that are not useful for our purpose. As an example, planktonic datasets contain many benthic animals, because larvae of benthic animals occur in the zooplankton (the so-called meroplankton). We cannot use the plankton datasets to infer anything about absence of benthos in the sea floor. 

From the dataset resulting from this action we harvest all potentially interesting datasets, that contain at least one benthic animal in the region of interest. We subsequently use the imis database with meta-information on the datasets, to list the meta-data of all these datasets. This results in the file ./data/derived_data/allDatasets.csv.

In this file we perform the (manual) selection of datasets to be used. We also qualify the datasets as either 'complete' or 'incomplete'. The result of this manual procedure is the file ./data/derived_data/allDatasets_selection.csv.


```{r requestData_step1, echo=TRUE,eval=FALSE}

# read geographic layers for plotting
layerurl <- paste0("http://geo.vliz.be/geoserver/MarineRegions/ows?service=WFS&version=1.0.0&",
                   "request=GetFeature&typeName=MarineRegions:eez_iho_union_v2&",
                   "outputFormat=application/json")
regions <- sf::st_read(layerurl)
 
# read selected geographic layers for downloading
roi <- read_delim(file.path(dataDir,"regions.csv"), delim = ",")

# check by plotting
regions %>% filter(mrgid %in% roi$mrgid) %>%
  ggplot() +
  geom_sf(fill = "blue", color = "white") +
  geom_sf_text(aes(label = mrgid), size = 2, color = "white") +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
ggsave(file.path(dataDir,"regionsOfInterest.png"), width = 3, height =  4, )
#== download data by geographic location and trait =====================================
beginDate<- "1900-01-01"
endDate <- "2020-05-31"
attributeID1 <- "Benthos"
# Full occurrence (selected columns)
for(ii in 1:length(roi$mrgid)){
  mrgid <- roi$mrgid[ii]
  print(paste("downloading data for", roi$marregion[ii]))
  
  downloadURL <- paste0(
  "http://geo.vliz.be/geoserver/wfs/ows?service=WFS&version=1.1.0&",
  "request=GetFeature&",
  "typeName=Dataportal%3Aeurobis-obisenv_full&",
  "resultType=results&",
  "viewParams=where%3A%28%28up.geoobjectsids+%26%26+ARRAY%5B",mrgid,
  "%5D%29%29+AND+%28%28observationdate+BETWEEN+%27",beginDate,
  "%27+AND+%27",endDate,
  "%27+%29%29+AND+aphiaid+IN+%28+SELECT+aphiaid+FROM+eurobis.taxa_attributes+WHERE+selectid+IN+%28%27",
  attributeID1,
  "%27%29%29%3Bcontext%3A0100&",
  "propertyName=datasetid%2C",
  "datecollected%2C",
  "decimallatitude%2C",
  "decimallongitude%2C",
  "scientificname%2C",
  "aphiaid%2C",
  "scientificnameaccepted%2C",
  "yearcollected%2C",
  "waterbody%2C",
  "country%2C",
  "recordnumber%2C",
  "fieldnumber%2C",
  "minimumdepthinmeters%2C",
  "maximumdepthinmeters%2C",
  "aphiaidaccepted%2C",
  "catalognumber%2C",
  "qc%2C",
  "eventid&",
  "outputFormat=csv")
  

  filename = paste0("region", roi$mrgid[ii], ".csv")
  data <- read_csv(downloadURL,col_types = "cnccTnccccnnnncccccn")
  write_delim(data, file.path(downloadDir, "byTrait", filename), delim = ",")
}
filelist <- list.files(file.path(downloadDir,"byTrait"))
allDataExtra <- lapply(filelist, function(x) 
  read_delim(file.path(downloadDir, "byTrait", x), 
             delim = ",", 
             col_types = "cnccTnccccnnnncccccn")) %>%
  set_names(sub(".csv", "", filelist)) %>%
  bind_rows(.id = "mrgid") %>%
  mutate(mrgid = sub("region", "", mrgid))
#write_delim(allDataExtra, file.path(dataDir, "allDataExtra.csv"), delim = ",")
 
#=== from downloaded data ===========================
#
#allDataExtra <- read_delim(file.path(dataDir, "allDataExtra.csv"), delim = ",")
datasetidsoi <- allDataExtra %>% distinct(datasetid) %>% 
  mutate(datasetid = as.numeric(sub('http://www.emodnet-biology.eu/data-catalog?module=dataset&dasid=', 
                         "", datasetid, fixed = T)))
#==== retrieve data by dataset ==============
#
# function to read dataset characteristics

fdr2<-function(dasid){
  datasetrecords <- datasets(dasid)
  dascitations <- getdascitations(datasetrecords)
  if(nrow(dascitations)==0)dascitations<-tibble(dasid=as.character(dasid),title="",citation="")
  if(nrow(dascitations)==1) if(is.na(dascitations$citation)) dascitations$citation<-""
  daskeywords <- getdaskeywords(datasetrecords)
  if(nrow(daskeywords)==0)daskeywords<-tibble(dasid=as.character(dasid),title="",keyword="")
  if(nrow(daskeywords)==1) if(is.na(daskeywords$keyword))daskeywords$keyword<-""
  dascontacts <- getdascontacts(datasetrecords)
  if(nrow(dascontacts)==0)dascontacts<-tibble(dasid=as.character(dasid),title="",contact="")
  if(nrow(dascontacts)==1) if(is.na(dascontacts$contact))dascontacts$contact<-""
  dastheme <- getdasthemes(datasetrecords)
  if(nrow(dastheme)==0)dastheme<-tibble(dasid=as.character(dasid),title="",theme="")
  if(nrow(dastheme)==1) if(is.na(dastheme$theme))dastheme$theme<-""
  dastheme2 <- aggregate(theme ~ dasid, data = dastheme, paste, 
                         collapse = " , ")
  daskeywords2 <- aggregate(keyword ~ dasid, data = daskeywords, 
                            paste, collapse = " , ")
  dascontacts2 <- aggregate(contact ~ dasid, data = dascontacts, 
                            paste, collapse = " , ")
  output <- dascitations %>% left_join(dascontacts2, by = "dasid") %>% 
    left_join(dastheme2, by = "dasid") %>% left_join(daskeywords2, 
                                                     by = "dasid")
  return(output)
}
all_info <- data.frame()
for (i in datasetidsoi$datasetid){
  dataset_info <- fdr2(i)
  all_info <- rbind(all_info, dataset_info)
}
names(all_info)[1]<-"datasetid"
write.csv(all_info,file=file.path(dataDir,"allDatasets.csv"),row.names = F)
# Note
# this step is followed by manual inspection of data sets, and selection
# results in file "./data/derived_data/allDatasets_selection.csv"
```
### Step 2. Download by dataset.
In this step, we download the part of all these useful datasets that occur in the region of interest. For practical reasons this region is subdivided in many subregions - in that way the downloaded files are not too big and there is less risk of interruptions of the process. After download, all these files will be recombined into one big datafile. 

```{r requestData_step2, echo=TRUE, eval=FALSE}
 
# content of script requestData_step2.R
 
# read selected geographic layers for downloading
roi <- read_delim(file.path(dataDir,"regions.csv"), delim = ",")
getDatasets <- read_csv(file.path(dataDir,"allDatasets_selection.csv"))
getDatasets <- getDatasets %>% filter(include)
for(ii in 1:length(roi$mrgid)){
  for(jj in 1:length(getDatasets$datasetid)){
    datasetid <- getDatasets$datasetid[jj]
    mrgid <- roi$mrgid[ii]
    print(paste("downloading data for ", roi$marregion[ii], "and dataset nr: ", datasetid))
    downloadURL <- paste0("https://geo.vliz.be/geoserver/wfs/ows?service=WFS&version=1.1.0",
      "&request=GetFeature&typeName=Dataportal%3Aeurobis-obisenv_full&resultType=results&",
      "viewParams=where%3A%28%28up.geoobjectsids+%26%26+ARRAY%5B", mrgid,
      "%5D%29%29+AND+datasetid+IN+(",datasetid,");context%3A0100&propertyName=datasetid%2C",
      "datecollected%2Cdecimallatitude%2Cdecimallongitude%2Ccoordinateuncertaintyinmeters%2C",
      "scientificname%2Caphiaid%2Cscientificnameaccepted%2Cinstitutioncode%2Ccollectioncode%2C",
      "occurrenceid%2Cscientificnameauthorship%2Cscientificnameid%2Ckingdom%2Cphylum%2Cclass",
      "%2Corder%2Cfamily%2Cgenus%2Csubgenus%2Caphiaidaccepted%2Cbasisofrecord%2Ceventid&",
      "outputFormat=csv")
    data <- read_csv(downloadURL, col_types = "ccccccTnnnccccccccccccccc") 
    filename = paste0("region", roi$mrgid[ii], "_datasetid", datasetid,  ".csv")
    if(nrow(data) != 0){
      write_delim(data, file.path(downloadDir, "byDataset", filename), delim = ",")
    }
  }
}
```

## step 3. Combine all downloaded datasets into one big dataset
In this step, we read in all the files written during the previous step, and combine the data into one big dataset to be used for further analysis and production of the maps. The procedure is contained in the script 'requestData_step3.R'. It is kept separate from the previous step because the downloading is very time-consuming and it can better be completed before this step takes place, including checks on errors, timeouts etc.
 
The procedure for this step is simple, and documented in the following code.
 
```{r requestData_step3, echo=TRUE,eval=FALSE}
filelist <- list.files(file.path(downloadDir,"byDataset"))
all2Data <- lapply(filelist, function(x) 
  read_delim(file.path(downloadDir,"byDataset", x), 
             delim = ",", 
             col_types = "ccccccTnnnccccccccccccccc"
			)
) %>%
  set_names(filelist) %>%
  bind_rows(.id = "fileID") %>%
  separate(fileID, c("mrgid", "datasetID"), "_") %>%
  mutate(mrgid = sub("[[:alpha:]]+", "", mrgid)) %>%
  mutate(datasetID = sub("[[:alpha:]]+", "", datasetID))
# mutate(mrgid = sub("region", "", mrgid))
all2Data<- all2Data %>%
  mutate(AphiaID=as.numeric(substr(aphiaidaccepted,52,65)))%>%
  filter(!is.na(AphiaID)) %>%
  filter(!is.na(decimallongitude)) %>%
  filter(!is.na(decimallatitude)) %>%
  filter(!is.na(datecollected))
write_delim(all2Data, file.path(dataDir, "all2Data.csv"), delim = ",")
```
 
## Analysis of the species represented in the dataset
The selection of data in the first step makes use of the traits stored in WoRMS, the World Register of Marine Species. For many species in this database, the functional group to which the species belongs is recorded. However, this is not yet complete. We can help the development of these traits databases from the compilation of data performed here. Since we selected benthic data sets, we can assume that most species in our database will be benthic, although it appears this is not absolutely the case everywhere. Here we try to use as much information as possible, either from the traits database or from the taxonomic position of the taxa, to derive what functional group they belong to. That is used to narrow down the list of taxa to the benthic species we are targeting, but also to report back to WoRMS with suggestions to improve the traits database. For example, one of the results of the present exercise was that it appeared not a single Amphipod is called 'benthos' in WoRMS. This has been flagged as a potential problem to the taxonomic editors of WoRMS.
 
The checks are illustrated in the script "select_species_4_maps.R". Note that the actual downloading of the trait information of over 6000 species is commented out. Results have been written to a file after performing this step once.
```{r select_species_4_maps, echo=TRUE,eval=FALSE}
# we build the species list, keeping the taxonomic information we have in the total data set
# we foresee logical columns in the species list to group the species by in the rest of this script
bns<-read_delim(file.path(dataDir,"all2Data.csv"),
                col_types = "ccccccccTnnncccccccccccccccn",
                delim=",")
splst <- bns %>% 
         select(AphiaID,scientificnameaccepted,phylum,class,order,family,genus,subgenus) %>% 
         distinct() %>%
         mutate(benthos=FALSE,endobenthos=FALSE,macrobenthos=FALSE,epibenthos=FALSE,
                meiobenthos=FALSE,phytobenthos=FALSE,
                plankton=FALSE,nekton=FALSE,Pisces=FALSE,Algae=FALSE,
                Aves_tax=FALSE,Pisces_tax=FALSE,Algae_tax=FALSE,Plants_tax=FALSE,
                meio_tax=FALSE,micro_tax=FALSE,misc_tax=FALSE)
# ###### determine, using attributes, which species are benthos #######
# ###### again, several hours download ##########
# (done once, result stored as delimited file)
#nsp_attr<-data.frame(AphiaID=NA,measurementTypeID=NA,measurementType=NA,
#                     measurementValue=NA,source_id=NA,reference=NA,qualitystatus=NA,
#                     AphiaID_Inherited=NA,CategoryID=NA)
nsp_attr<-tibble()
for(i in 1:nrow(splst)){
  print(paste(i,"out of",nrow(splst),"downloading attributes of species",
              splst$scientificnameaccepted[i],"AphiaID",splst$AphiaID[i]))
  ttt<-NULL
  try(ttt<-wm_attr_data(id=splst$AphiaID[i],include_inherited = T),silent = T)
  if(! is.null(ttt)) nsp_attr<-rbind(nsp_attr,ttt[,1:9])
}

nsp_attr <- nsp_attr %>%
                  mutate(AphiaID=as.numeric(AphiaID)) %>%
                  left_join(splst,by="AphiaID")
write_delim(nsp_attr,file.path(dataDir,"nsp_attr.Rdata"),delim=",")
#
#nsp_attr <- read_delim(file.path(dataDir,"nsp_attr.csv"),delim=",")
#
# what Functional groups are there?
fg <- nsp_attr %>% filter(measurementType=="Functional group") %>% 
                   select(measurementValue) %>% 
                   distinct
print(fg)
# what Paraphyletic groups are there?
pfg <- nsp_attr %>% filter(measurementType=="Paraphyletic group") %>% 
                    select(measurementValue) %>% 
                    distinct
print(pfg)
# fill in attributes columns of splst based on the attributes downloaded from WoRMS
set_attr<-function(attr){
  tt <- nsp_attr                               %>% 
          filter(grepl(attr,measurementValue)) %>% 
          select(AphiaID)                      %>% 
          distinct()
  splst <- splst %>% 
             mutate(!!attr:=ifelse(AphiaID %in% tt$AphiaID,TRUE,FALSE))
  return(splst)
}
splst<-set_attr("benthos")
splst<-set_attr("endobenthos")
splst<-set_attr("macrobenthos")
splst<-set_attr("epibenthos")
splst<-set_attr("meiobenthos")
splst<-set_attr("phytobenthos")
splst<-set_attr("Pisces")
splst<-set_attr("Algae")
splst<-set_attr("plankton")
splst<-set_attr("nekton")
# fill in attributes columns based on taxonomic information
splst$Pisces_tax <- splst$Pisces_tax | splst$class  == "Actinopterygii" 
splst$Pisces_tax <- splst$Pisces_tax | splst$class  == "Elasmobranchii"
splst$Aves_tax   <- splst$Aves_tax   | splst$class  == "Aves"
splst$Algae_tax  <- splst$Algae_tax  | splst$phylum == "Chlorophyta"
splst$Algae_tax  <- splst$Algae_tax  | splst$phylum == "Rhodophyta"
splst$Algae_tax  <- splst$Algae_tax  | splst$phylum == "Ochrophyta"
splst$Algae_tax  <- splst$Algae_tax  | splst$phylum == "Charophyta"
splst$Algae_tax  <- splst$Algae_tax  | splst$phylum == "Cyanobacteria"
splst$Algae_tax  <- splst$Algae_tax  | splst$phylum == "Haptophyta"
splst$Plants_tax <- splst$Plants_tax | splst$Algae_tax 
splst$Plants_tax <- splst$Plants_tax | splst$phylum == "Tracheophyta"
splst$Plants_tax <- splst$Plants_tax | splst$phylum == "Bryophyta"
splst$micro_tax  <- splst$micro_tax | splst$phylum == "Ascomycota"
splst$micro_tax  <- splst$micro_tax | splst$phylum == "Proteobacteria"
splst$meio_tax   <- splst$meio_tax  | splst$phylum == "Nematoda"
splst$meio_tax   <- splst$meio_tax  | splst$phylum == "Foraminifera"
splst$meio_tax   <- splst$meio_tax  | splst$phylum == "Tardigrada"
splst$meio_tax   <- splst$meio_tax  | splst$phylum == "Gastrotricha"
splst$meio_tax   <- splst$meio_tax  | splst$phylum == "Kinorhyncha"
splst$meio_tax   <- splst$meio_tax  | splst$phylum == "Ciliophora"
splst$meio_tax   <- splst$meio_tax  | splst$class  == "Ostracoda"
splst$meio_tax   <- splst$meio_tax  | splst$order  == "Harpacticoida"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Arachnida"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Mammalia"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Insecta"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Ichthyostraca"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Diplopoda"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Collembola"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Chilopoda"
splst$misc_tax   <- splst$misc_tax  | splst$class  == "Clitellata"
# write splst to output
write_delim(splst,file.path(dataDir,"splst.csv"),delim=",")
# lists to be produced for WoRMS people
# list of fish species that do not have Paraphyletic group == Pisces
prob1 <- splst %>% filter (Pisces_tax & !Pisces)
write_delim(prob1,file.path(dataDir,"specieslist1.csv"),delim=",")
# list of algae species that do not have Paraphyletic group == Algae
prob2 <- splst %>% filter (Algae_tax & !Algae)
write_delim(prob2,file.path(dataDir,"specieslist2.csv"),delim=",")
# list of species that should have a paraphyletic group 'plants' or something
prob3 <- splst %>% filter (Plants_tax)
write_delim(prob3,file.path(dataDir,"specieslist3.csv"),delim=",")
# list of species that are likely meiobenthos (based on taxonomy) but no attribute meiobenthos
prob4 <- splst %>% filter (meio_tax & !meiobenthos)
write_delim(prob4,file.path(dataDir,"specieslist4.csv"),delim=",")
# list of bird species that maybe should get a Paraphyletic group 'Aves'
prob5 <- splst %>% filter (Aves_tax)
write_delim(prob5,file.path(dataDir,"specieslist5.csv"),delim=",")
# list of species that are classified as 'nekton' but are sometimes considered benthic
prob6 <- splst %>% filter (nekton)
write_delim(prob6,file.path(dataDir,"specieslist6.csv"),delim=",")
# list of species of odd taxa that do not really belong in benthos studies
prob7 <- splst %>% filter (misc_tax & !benthos)
write_delim(prob7,file.path(dataDir,"specieslist7.csv"),delim=",")
# list of species found in benthic datasets, but that are not benthos, not fish, not birds, 
# not plants, not micro-organisms, not meiofauna, not plankton and not nekton
prob8 <- splst %>% filter (!benthos&!Pisces&!Pisces_tax&!Aves_tax&!Plants_tax&!Algae&
                             !micro_tax&!meio_tax&!meiobenthos&!plankton&!nekton) %>%
  arrange(phylum,class,order,family,genus,subgenus,scientificnameaccepted)
write_delim(prob8,file.path(dataDir,"specieslist8.csv"),delim=",")
####### So, what species to use for the maps? #########################
# species should be:
# * not meiobenthos or meio_tax
# * not phytobenthos
# * not Pisces or Pisces_tax
# * not Plants_tax (which includes Algae_tax)
# * not Algae
# * not micro_tax
# * not Aves_tax
# * not misc_tax
# * not plankton (if they are not either benthos or nekton too)
sp2use <- splst %>% 
  filter (!meiobenthos & !meio_tax & !phytobenthos & !Pisces & !Pisces_tax & 
            !Plants_tax & !Algae & !micro_tax & ! Aves_tax & 
            !(plankton & !(benthos|nekton)) &
            !(misc_tax & !benthos))
write_delim(sp2use,file.path(dataDir,"sp2use.csv"),delim=",")
```


## Production of rasters with presence/absence information

For the production of maps, we define 'sampling events' as the ensembles of records that share time and place. We consider such events as one sample. For the incomplete datasets, we inventory what species they have targeted. Finally, for every species we determine whether or not it was present in all sampling events of all relevant datasets. This presence/absence information is written to the output file, together with the spatial location and the sampling date. The intention is to use this information to produce interpolated maps covering also the non-sampled space. As a first step in visualisation, we rasterize this information and show it in a map per species. In the following code block, the rasters are calculated and stored in one file per species (for all species that were found more than 200 times - this could easily be extended to all species).

```{r calculate_rasters, echo=TRUE, eval=FALSE}
##########################################################
#### load data
##########################################################
sbnsc<- read_delim(file.path(dataDir,"all2Data.csv"),
                   col_types = "ccccccccTnnncccccccccccccccn",
                   delim=",")
trdi<-read.csv("./data/derived_data/allDatasets_selection.csv",
               stringsAsFactors = FALSE)
usedds<- trdi %>% filter (include) %>% dplyr::select(datasetid)
splst<-read_delim(file.path(dataDir,"sp2use.csv"),
                  col_types = "dccccccclllllllllllllllll",
                  delim=",")
##########################################################
##### select few columns to work with
##### filter to only the used datasets
##### and filter to true benthic species only
##########################################################
trec<- sbnsc %>% dplyr::select(eventDate=datecollected,
                               decimalLongitude=decimallongitude,
                               decimalLatitude=decimallatitude,
                               scientificName=scientificnameaccepted,
                               aphiaID=AphiaID,
                               datasetid=datasetid) %>%
  mutate(datasetid=as.numeric(substr(datasetid,65,90))) %>%
  filter(datasetid %in% usedds$datasetid)
trec<- trec %>%  filter(aphiaID %in% splst$AphiaID)
##############################################################
# Define 'sampling events' as all records that share time and place, give
# ID numbers to all events (eventNummer), and store the eventNummer in each
# record of trec
##############################################################
events<- trec %>% dplyr::select(eventDate,decimalLongitude,decimalLatitude) %>% 
  distinct() %>%
  mutate(eventNummer=row_number())
trec <- trec %>% left_join(events,by=c('eventDate','decimalLongitude','decimalLatitude'))
########### work on datasets
#
#### check on completeness
#
nsp<-trec %>% group_by(datasetid) %>% 
  distinct(aphiaID)    %>%
  mutate(nspec=n())    %>%
  dplyr::select(datasetid,nspec) %>%
  distinct()
nev<-trec %>% group_by(datasetid)     %>% 
  distinct(eventNummer)    %>%
  mutate(nev=n())          %>%
  dplyr::select(datasetid,nev) %>%
  distinct()                    %>%
  left_join(nsp,by='datasetid')%>%
  left_join(trdi,by='datasetid')
#
plot(nev$nev,nev$nspec,log="xy",col=ifelse(nev$complete,"blue","red"),pch=19,
     xlab="number of events in dataset",ylab="number of species in dataset")
text(nev$nev*1.2,nev$nspec*(1+(runif(nrow(nev))-0.5)*0.4),nev$datasetid,cex=0.5)
# 
# #notes. BIS (599) and Voordelta (4662) rather poor in species for the effort. is OK
# #       similar 5701 Belgian coast and 1794 French coast
# #       In the end we only keep four incomplete datasets
# 
# 
# manage the incomplete datasets
#
trdi_ct<-trdi %>% filter (complete)
trdi_ic<-trdi %>% filter (!complete)
# make a species list for each incomplete dataset
ic_sp<-data.frame(datasetid=NULL,aphiaID=NULL)
for(i in 1:nrow(trdi_ic)){
  ds<-trdi_ic$datasetid[i]
  specs<-unique(trec$aphiaID[trec$datasetid==ds])
  ic_sp<-rbind(ic_sp,data.frame(datasetid=rep(ds,length(specs)),aphiaID=specs))
}
##############################################################
# find occurrence frequency of all species, and rank the species accordingly
#
spfr<- trec %>% 
  group_by(aphiaID,scientificName) %>%
  summarize(n_events=n()) %>%
  arrange(desc(n_events))
nsptoplot<-length(which(spfr$n_events>200))
############ end of the generic part. What follows is a loop over the species ##
spmin<-1
spmax<-nsptoplot
pb <- progress_bar$new(total=nsptoplot)

for(ss in spmin:spmax){
  spAphId<-spfr$aphiaID[ss]
  specname<-spfr$scientificName[ss]
  spcolumn<-paste0("pa",spAphId)
  pb$tick()
  # from the list of incomplete datasets, check if they have our species. 
  # Only keep these, drop the others
  tt_ds<- ic_sp                      %>% 
    filter(aphiaID==spAphId) %>%
    distinct(datasetid)      %>% 
    bind_rows(trdi_ct        %>% 
                dplyr::select(datasetid))
  # The dataset to be used consists of all complete datasets, and all 
  # incomplete datasets that targeted our species
  spe<- trec                                                      %>% 
    filter(datasetid %in% tt_ds$datasetid)                  %>%
    group_by(eventNummer)                                   %>%
    summarize(pres_abs= as.numeric(any(aphiaID==spAphId)),.groups = 'drop')  %>%
    left_join(events,by='eventNummer') 
  spesh <- spe                               %>% 
    mutate (spcolumn=(pres_abs==1)) %>% 
    dplyr::select (- pres_abs)
  names(spesh)[5]<-spcolumn
  spesh <- spesh %>% dplyr::select('eventNummer',spcolumn)
  if(ss==spmin) allspe <- spesh else {
    allspe <- allspe %>% full_join(spesh,by='eventNummer')
  }
  coordinates(spe)<- ~decimalLongitude+decimalLatitude
  projection(spe)<-proWG
  r1<-rasterize(spe,r,field="pres_abs",fun=mean)
  # Export rasters as tif
  raster::writeRaster(
    r1, 
    file.path(
      rasterDir, paste0(
        sprintf("%04d",ss), "_",
        spAphId, "_",
        gsub(" ", "-", specname),
        ".tif"
      )
    ),
    overwrite=TRUE
  )
}

# save file with presence/absence information of all species plotted
evs<-tibble(eventNummer=allspe$eventNummer)
evs<- evs %>% left_join(events,by='eventNummer')
spe<- cbind(evs,allspe[,2:ncol(allspe)])
save(spe,file=file.path(mapsDir,"spe.Rdata"))
write_delim(spfr,path=file.path(mapsDir,"specieslist.csv"),delim=",")
for(i in 5:ncol(spe)) spe[,i]<-as.numeric(spe[,i])
write_delim(spe,path=file.path(mapsDir,"spe.csv"),delim=",")

```
##Production of maps of the rasters
We use the EMODnet mapping package EMODnetBiologyMaps to produce maps of the rasters.

```{r produce_plots_with_emodnet_package, eval=FALSE}

### correct the EMODnetBiologyMaps package by running a slightly modified version
emodnet_map_plot_2<-function (data, fill = NULL, title = NULL, subtitle = NULL, 
                              legend = NULL, crs = 3035, xlim = c(2426378.0132, 
                              7093974.6215), ylim = c(1308101.2618, 5446513.5222), 
                              direction = 1, option = "viridis",zoom = FALSE, ...) 
{
  stopifnot(class(data)[1] %in% c("sf", "RasterLayer", "SpatialPolygonsDataFrame", 
                                  "SpatialPointsDataFrame","SpatialLinesDataFrame"))
  emodnet_map_basic <- emodnet_map_basic(...) + ggplot2::ggtitle(label = title, 
                                                                 subtitle = subtitle)
  if (class(data)[1] == "RasterLayer") {
    message("Transforming RasterLayer to sf vector data")
    data <- sf::st_as_sf(raster::rasterToPolygons(data))
    fill <- sf::st_drop_geometry(data)[, 1]
  }
  if (class(data)[1] %in% c("SpatialPolygonsDataFrame", 
                            "SpatialPointsDataFrame", "SpatialLinesDataFrame")) {
    message("Transforming sp to sf")
    data <- sf::st_as_sf(data)
  }
  if (sf::st_geometry_type(data, FALSE) == "POINT") {
    emodnet_map_plot <- emodnet_map_basic + ggplot2::geom_sf(data = data, 
                          color = emodnet_colors()$yellow)
  }
  if (sf::st_geometry_type(data, FALSE) == "POLYGON") {
    emodnet_map_plot <- emodnet_map_basic + 
      ggplot2::geom_sf(data = data,ggplot2::aes(fill = fill),
                       size = 0.05,color=NA) + 
      ggplot2::scale_fill_viridis_c(alpha = 0.8,                                                                                    name = legend, direction = direction, option = option)
  }
  if (zoom == TRUE) {
    bbox <- sf::st_bbox(sf::st_transform(data, crs))
    xlim <- c(bbox$xmin, bbox$xmax)
    ylim <- c(bbox$ymin, bbox$ymax)
  }
  emodnet_map_plot <- emodnet_map_plot + ggplot2::coord_sf(crs = crs, 
                                                    xlim = xlim, ylim = ylim)
  return(emodnet_map_plot)
}
########## end of modified plot function - eventually to be moved into the package


#### load specieslist
spfr<-read_delim(file.path(mapsDir,"specieslist.csv"),delim=",")
nsptoplot<-length(which(spfr$n_events>200))
spmin<-1
spmax<-nsptoplot
pb <- progress_bar$new(total=nsptoplot)
#########################################################
for(ss in spmin:spmax){
  pb$tick()
  spAphId<-spfr$aphiaID[ss]
  specname<-spfr$scientificName[ss]
  rasterfil <- file.path(rasterDir, 
     paste0(sprintf("%04d",ss), "_",spAphId, "_",gsub(" ", "-", specname),".tif"))
  r1<-raster(rasterfil)  

  legend="P(pres)"
  # Plot the grid
  
  ec<-emodnet_colors()
  plot_grid <- emodnet_map_plot_2(data=r1,title=specname,subtitle=paste0('AphiaID ', spAphId),
                                  zoom=TRUE,seaColor=ec$darkgrey,landColor=ec$lightgrey,legend=legend)
  filnam<-file.path(plotsDir, 
                    paste0(sprintf("%04d",ss), "_",spAphId, "_",gsub(" ", "-", specname),".png"))
  
  emodnet_map_logo(plot_grid,path=filnam,width=120,height=160,dpi=300,units="mm",offset="+0+0")
  
}


```
